{
    "enocding_dict" : "/data/tzeshinchen/research/gpt2/tokenizer/encoder.json",
    "decoding_dict" : "/data/tzeshinchen/research/gpt2/tokenizer/decoder.json",
    "training_data" : "/data/tzeshinchen/research/dataset/smiles_pad.txt",
    "device" : "'cuda' if torch.cuda.is_available() else 'cpu'",
    "batch_size" : 256,
    "block_size" : 180,
    "max_iters" : 30000,
    "learning_rate" : 1e-4,
    "eval_iters" : 200,
    "n_embd" : 384,
    "n_head" : 18,
    "n_layer" : 12,
    "dropout" : 0.2,
    "vocab_size" : 63,
    "training_size" : 1100000,
    "warnup_steps" : 2000
}